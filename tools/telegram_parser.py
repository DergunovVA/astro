"""
Telegram Channel Parser –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—É–±–ª–∏—á–Ω—ã–µ Telegram –∫–∞–Ω–∞–ª—ã –∞—Å—Ç—Ä–æ–ª–æ–≥–æ–≤:
- –ß–∞—Å—Ç–æ—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
- –¢–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –ø–æ—Å—Ç–æ–≤
- –¢–µ–º—ã –∏ —Å—Ç–∏–ª—å
"""

import os
import json
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from collections import Counter
from telethon.sync import TelegramClient
from telethon.tl.functions.messages import GetHistoryRequest
from telethon.tl.types import MessageMediaPhoto, MessageMediaDocument

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (—Å–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env –∏–ª–∏ –ø–µ—Ä–µ–¥–∞–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ)
API_ID = os.getenv("TELEGRAM_API_ID")  # –ü–æ–ª—É—á–∏—Ç—å –Ω–∞ my.telegram.org
API_HASH = os.getenv("TELEGRAM_API_HASH")
PHONE = os.getenv("TELEGRAM_PHONE")  # –í–∞—à –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞

# –ö–∞–Ω–∞–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
ASTRO_CHANNELS = {
    "chayka": "@astro_chayka",  # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω –î–∞—Ä–∞–≥–∞–Ω (–ß–∞–π–∫–∞)
    "kulkov": "@alexeykoulkov",  # –ê–ª–µ–∫—Å–µ–π –ö—É–ª—å–∫–æ–≤
    "andreev": "@astrolaboratory",  # –ü–∞–≤–µ–ª –ê–Ω–¥—Ä–µ–µ–≤ (–ø—Ä–∏–º–µ—Ä–Ω—ã–π)
    "bunyakova": "@astrologforyou",  # –ò–Ω–Ω–∞ –ë—É–Ω—è–∫–æ–≤–∞
    "globa": "@globa_astro",  # –ü–∞–≤–µ–ª –ì–ª–æ–±–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
}


class TelegramChannelAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä Telegram –∫–∞–Ω–∞–ª–æ–≤ –∞—Å—Ç—Ä–æ–ª–æ–≥–æ–≤"""

    def __init__(self, api_id: str, api_hash: str, phone: str):
        self.client = TelegramClient("astro_competitor_session", api_id, api_hash)
        self.phone = phone
        self.data_dir = Path("data/competitors")
        self.data_dir.mkdir(parents=True, exist_ok=True)

    async def connect(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Telegram"""
        await self.client.start(phone=self.phone)
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Telegram")

    async def get_channel_info(self, channel_username: str):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–Ω–∞–ª–µ"""
        try:
            entity = await self.client.get_entity(channel_username)
            return {
                "id": entity.id,
                "title": entity.title,
                "username": entity.username,
                "participants_count": getattr(entity, "participants_count", "N/A"),
                "description": getattr(entity, "about", "N/A"),
            }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è info –¥–ª—è {channel_username}: {e}")
            return None

    async def parse_channel_posts(
        self, channel_username: str, limit: int = 100, days_back: int = 30
    ):
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –ø–æ—Å—Ç–æ–≤ –∫–∞–Ω–∞–ª–∞

        Args:
            channel_username: @username –∫–∞–Ω–∞–ª–∞
            limit: –º–∞–∫—Å–∏–º—É–º –ø–æ—Å—Ç–æ–≤
            days_back: —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ —Å–º–æ—Ç—Ä–µ—Ç—å
        """
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–∞: {channel_username}")

        try:
            entity = await self.client.get_entity(channel_username)
            posts = []
            offset_date = datetime.now() - timedelta(days=days_back)

            # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π
            offset_id = 0
            while len(posts) < limit:
                history = await self.client(
                    GetHistoryRequest(
                        peer=entity,
                        offset_id=offset_id,
                        offset_date=offset_date,
                        add_offset=0,
                        limit=100,
                        max_id=0,
                        min_id=0,
                        hash=0,
                    )
                )

                if not history.messages:
                    break

                for message in history.messages:
                    if message.date < offset_date:
                        break

                    post_data = self._extract_post_data(message)
                    if post_data:
                        posts.append(post_data)

                offset_id = history.messages[-1].id

                if len(history.messages) < 100:
                    break

            print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}")
            return posts

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {channel_username}: {e}")
            return []

    def _extract_post_data(self, message):
        """–ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–æ—Å—Ç–∞"""
        if not message or not message.message:
            return None

        return {
            "id": message.id,
            "date": message.date.isoformat(),
            "text": message.message,
            "text_length": len(message.message),
            "views": message.views or 0,
            "forwards": message.forwards or 0,
            "replies": message.replies.replies if message.replies else 0,
            "reactions": self._extract_reactions(message),
            "media_type": self._get_media_type(message),
            "links": self._extract_links(message),
            "hashtags": self._extract_hashtags(message),
        }

    def _extract_reactions(self, message):
        """–ò–∑–≤–ª–µ—á—å —Ä–µ–∞–∫—Ü–∏–∏ –Ω–∞ –ø–æ—Å—Ç"""
        if not hasattr(message, "reactions") or not message.reactions:
            return {}

        reactions = {}
        for reaction in message.reactions.results:
            emoji = getattr(reaction.reaction, "emoticon", "unknown")
            reactions[emoji] = reaction.count

        return reactions

    def _get_media_type(self, message):
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –º–µ–¥–∏–∞ –≤ –ø–æ—Å—Ç–µ"""
        if not message.media:
            return "text"

        if isinstance(message.media, MessageMediaPhoto):
            return "photo"
        elif isinstance(message.media, MessageMediaDocument):
            if message.media.document.mime_type.startswith("video"):
                return "video"
            return "document"

        return "other"

    def _extract_links(self, message):
        """–ò–∑–≤–ª–µ—á—å —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        import re

        url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
        return re.findall(url_pattern, message.message)

    def _extract_hashtags(self, message):
        """–ò–∑–≤–ª–µ—á—å —Ö–µ—à—Ç–µ–≥–∏"""
        import re

        hashtag_pattern = r"#\w+"
        return re.findall(hashtag_pattern, message.message)

    def analyze_posts(self, posts: list, channel_name: str):
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤"""
        if not posts:
            print("‚ö†Ô∏è –ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return None

        print(f"\nüìà –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∫–∞–Ω–∞–ª–∞ {channel_name}:")

        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_posts = len(posts)
        total_views = sum(p["views"] for p in posts)
        avg_views = total_views / total_posts if total_posts > 0 else 0

        # –¢–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        media_types = Counter(p["media_type"] for p in posts)

        # –î–ª–∏–Ω–∞ –ø–æ—Å—Ç–æ–≤
        avg_length = sum(p["text_length"] for p in posts) / total_posts

        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –ø–æ—Å—Ç—ã
        top_posts = sorted(posts, key=lambda x: x["views"], reverse=True)[:5]

        # –ß–∞—Å—Ç–æ—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
        dates = [datetime.fromisoformat(p["date"]).date() for p in posts]
        date_counts = Counter(dates)
        avg_posts_per_day = len(posts) / len(set(dates)) if dates else 0

        # –†–µ–∞–∫—Ü–∏–∏
        all_reactions = {}
        for post in posts:
            for emoji, count in post["reactions"].items():
                all_reactions[emoji] = all_reactions.get(emoji, 0) + count

        # –•–µ—à—Ç–µ–≥–∏
        all_hashtags = []
        for post in posts:
            all_hashtags.extend(post["hashtags"])
        top_hashtags = Counter(all_hashtags).most_common(10)

        analysis = {
            "channel_name": channel_name,
            "period_days": (max(dates) - min(dates)).days if dates else 0,
            "statistics": {
                "total_posts": total_posts,
                "total_views": total_views,
                "avg_views_per_post": round(avg_views, 0),
                "avg_length_chars": round(avg_length, 0),
                "avg_posts_per_day": round(avg_posts_per_day, 2),
            },
            "content_types": dict(media_types),
            "top_reactions": dict(
                sorted(all_reactions.items(), key=lambda x: x[1], reverse=True)[:5]
            ),
            "top_hashtags": dict(top_hashtags),
            "top_posts": [
                {
                    "id": p["id"],
                    "date": p["date"],
                    "views": p["views"],
                    "text_preview": p["text"][:200] + "..."
                    if len(p["text"]) > 200
                    else p["text"],
                    "media": p["media_type"],
                }
                for p in top_posts
            ],
        }

        # –í—ã–≤–æ–¥
        print(f"  üìä –í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {total_posts}")
        print(f"  üëÅÔ∏è –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {avg_views:,.0f}")
        print(f"  üìù –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  üìÖ –ü–æ—Å—Ç–æ–≤ –≤ –¥–µ–Ω—å: {avg_posts_per_day:.1f}")
        print(f"  üì∑ –¢–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {dict(media_types)}")
        print(f"  ‚ù§Ô∏è –¢–æ–ø —Ä–µ–∞–∫—Ü–∏–∏: {dict(list(all_reactions.items())[:3])}")

        return analysis

    def analyze_content_themes(self, posts: list):
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)"""
        print("\nüîç –ê–Ω–∞–ª–∏–∑ —Ç–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∞—Å—Ç—Ä–æ–ª–æ–≥–∏–∏
        keywords = {
            "—Ö–æ—Ä–∞—Ä–Ω–∞—è": 0,
            "–Ω–∞—Ç–∞–ª—å–Ω–∞—è": 0,
            "—Ç—Ä–∞–Ω–∑–∏—Ç": 0,
            "–ø—Ä–æ–≥–Ω–æ–∑": 0,
            "—Ä–µ—Ç—Ä–æ–≥—Ä–∞–¥": 0,
            "–º–µ—Ä–∫—É—Ä–∏–π": 0,
            "–≤–µ–Ω–µ—Ä–∞": 0,
            "–º–∞—Ä—Å": 0,
            "—é–ø–∏—Ç–µ—Ä": 0,
            "—Å–∞—Ç—É—Ä–Ω": 0,
            "–ª—É–Ω–∞": 0,
            "—Å–æ–ª–Ω—Ü–µ": 0,
            "–∞—Å–ø–µ–∫—Ç": 0,
            "–¥–æ–º": 0,
            "–∑–Ω–∞–∫": 0,
            "–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è": 0,
            "–∫—É—Ä—Å": 0,
            "–æ–±—É—á–µ–Ω–∏–µ": 0,
            "–≤–æ–ø—Ä–æ—Å": 0,
            "–æ—Ç–≤–µ—Ç": 0,
        }

        for post in posts:
            text_lower = post["text"].lower()
            for keyword in keywords:
                if keyword in text_lower:
                    keywords[keyword] += 1

        # –¢–æ–ø —Ç–µ–º—ã
        top_themes = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:10]

        for theme, count in top_themes:
            percentage = (count / len(posts)) * 100
            print(f"  ‚Ä¢ {theme}: {count} –ø–æ—Å—Ç–æ–≤ ({percentage:.1f}%)")

        return dict(top_themes)

    async def save_results(self, channel_name: str, analysis: dict, posts: list):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑
        analysis_file = self.data_dir / f"{channel_name}_analysis_{timestamp}.json"
        with open(analysis_file, "w", encoding="utf-8") as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
        posts_file = self.data_dir / f"{channel_name}_posts_{timestamp}.json"
        with open(posts_file, "w", encoding="utf-8") as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)

        print("\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"  ‚Ä¢ {analysis_file}")
        print(f"  ‚Ä¢ {posts_file}")

    async def compare_channels(self, channels_data: dict):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞–Ω–∞–ª–æ–≤"""
        print("\nüîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞–Ω–∞–ª–æ–≤:\n")

        comparison = []
        for name, data in channels_data.items():
            comparison.append(
                {
                    "channel": name,
                    "posts": data["statistics"]["total_posts"],
                    "avg_views": data["statistics"]["avg_views_per_post"],
                    "posts_per_day": data["statistics"]["avg_posts_per_day"],
                    "avg_length": data["statistics"]["avg_length_chars"],
                }
            )

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º
        comparison.sort(key=lambda x: x["avg_views"], reverse=True)

        print("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
        print("‚îÇ –ö–∞–Ω–∞–ª       ‚îÇ –ü–æ—Å—Ç–æ–≤ ‚îÇ –°—Ä.–ø—Ä–æ—Å–º–æ—Ç—Ä—ã‚îÇ –ü–æ—Å—Ç/–¥–µ–Ω—å    ‚îÇ –°—Ä.–¥–ª–∏–Ω–∞   ‚îÇ")
        print("‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")

        for ch in comparison:
            print(
                f"‚îÇ {ch['channel']:11} ‚îÇ {ch['posts']:6} ‚îÇ {ch['avg_views']:11,.0f} ‚îÇ {ch['posts_per_day']:12.2f} ‚îÇ {ch['avg_length']:10.0f} ‚îÇ"
            )

        print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")

        return comparison

    async def disconnect(self):
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç Telegram"""
        await self.client.disconnect()
        print("\n‚úÖ –û—Ç–∫–ª—é—á–µ–Ω–æ –æ—Ç Telegram")


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ API –∫–ª—é—á–µ–π
    if not all([API_ID, API_HASH, PHONE]):
        print("‚ùå –û—à–∏–±–∫–∞: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:")
        print("   TELEGRAM_API_ID - –ø–æ–ª—É—á–∏—Ç—å –Ω–∞ https://my.telegram.org")
        print("   TELEGRAM_API_HASH")
        print("   TELEGRAM_PHONE - –≤–∞—à –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞")
        return

    # –°–æ–∑–¥–∞—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = TelegramChannelAnalyzer(API_ID, API_HASH, PHONE)

    try:
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        await analyzer.connect()

        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞
        all_analyses = {}

        for name, username in ASTRO_CHANNELS.items():
            print(f"\n{'=' * 60}")
            print(f"–ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–∞: {name} ({username})")
            print("=" * 60)

            # –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–Ω–∞–ª–µ
            info = await analyzer.get_channel_info(username)
            if info:
                print(f"üì¢ {info['title']}")
                print(f"üë• –ü–æ–¥–ø–∏—Å—á–∏–∫–æ–≤: {info.get('participants_count', 'N/A')}")

            # –ü–∞—Ä—Å–∏–Ω–≥ –ø–æ—Å—Ç–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π, –¥–æ 200 –ø–æ—Å—Ç–æ–≤)
            posts = await analyzer.parse_channel_posts(
                username, limit=200, days_back=30
            )

            if posts:
                # –ê–Ω–∞–ª–∏–∑
                analysis = analyzer.analyze_posts(posts, name)
                themes = analyzer.analyze_content_themes(posts)

                if analysis:
                    analysis["themes"] = themes
                    analysis["channel_info"] = info
                    all_analyses[name] = analysis

                    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    await analyzer.save_results(name, analysis, posts)

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(2)

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–∞–Ω–∞–ª–æ–≤
        if len(all_analyses) > 1:
            comparison = await analyzer.compare_channels(all_analyses)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
            comparison_file = (
                analyzer.data_dir
                / f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            with open(comparison_file, "w", encoding="utf-8") as f:
                json.dump(
                    {"comparison": comparison, "full_data": all_analyses},
                    f,
                    ensure_ascii=False,
                    indent=2,
                )

            print(f"\nüíæ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {comparison_file}")

        # –í—ã–≤–æ–¥—ã
        print("\n" + "=" * 60)
        print("üìä –í–´–í–û–î–´ –î–õ–Ø –ü–†–û–î–£–ö–¢–ê:")
        print("=" * 60)

        if all_analyses:
            # –°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –∫–∞–Ω–∞–ª
            best = max(
                all_analyses.items(),
                key=lambda x: x[1]["statistics"]["avg_views_per_post"],
            )
            print(f"\nüèÜ –°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π: {best[0]}")
            print(
                f"   –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {best[1]['statistics']['avg_views_per_post']:,.0f}"
            )

            # –°–∞–º—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π
            most_active = max(
                all_analyses.items(),
                key=lambda x: x[1]["statistics"]["avg_posts_per_day"],
            )
            print(f"\n‚ö° –°–∞–º—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π: {most_active[0]}")
            print(
                f"   –ü–æ—Å—Ç–æ–≤ –≤ –¥–µ–Ω—å: {most_active[1]['statistics']['avg_posts_per_day']:.2f}"
            )

            # –û–±—â–∏–µ —Ç–µ–º—ã
            all_themes = {}
            for analysis in all_analyses.values():
                for theme, count in analysis.get("themes", {}).items():
                    all_themes[theme] = all_themes.get(theme, 0) + count

            print("\nüî• –¢–æ–ø —Ç–µ–º—ã (—á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç):")
            for theme, count in sorted(
                all_themes.items(), key=lambda x: x[1], reverse=True
            )[:5]:
                print(f"   ‚Ä¢ {theme}: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")

    finally:
        await analyzer.disconnect()


if __name__ == "__main__":
    asyncio.run(main())
